# ğŸ§  RAG Pipeline with LangChain, Ollama, and Chroma

This project implements a Retrieval-Augmented Generation (RAG) system using **LangChain**, **Chroma vector store**, and **Ollama-powered LLMs**. It processes local documents, chunks and embeds them, and enables intelligent question answering over your data.

---

## ğŸš€ Features

- Hybrid document chunking (semantic + token-based)
- Multi-query retriever with LLM-generated reformulations
- Context-aware RAG chain construction
- Supports `.pdf`, `.txt`, `.md`, and `.docx` files
- Local embeddings via Ollama + ChromaDB
- Conversational Q&A loop via CLI

---

## ğŸ“ Directory Structure

â”œâ”€â”€ data/ # Source documents
â”œâ”€â”€ embedder.py # Embedding and vector store logic
â”œâ”€â”€ file_process.py # Document loading and chunking
â”œâ”€â”€ retriever.py # Multi-query retriever logic
â”œâ”€â”€ main.py # App entry point
â”œâ”€â”€ constants.py # Configurable constants
â”œâ”€â”€ index/ # ChromaDB persistent storage
â””â”€â”€ README.md # You're here

---

## âš™ï¸ Setup & Installation

1. **Clone the repository**

   ```bash
   git clone <your-repo-url>
   cd <your-project-folder>

   ```

2. **Install dependencies**
   pip install -r requirements.txt

3. **Install Ollama & run your LLM**
   ollama run _your-llm_

4. **Add documents to your /data folder**

## ğŸ§© Running the RAG APP

    python main.py

## ğŸ›  Key Components

- Chunking: Uses unstructured, chunk_by_title, and LangChain's RecursiveCharacterTextSplitter
- Vector Store: ChromaDB with OllamaEmbeddings (e.g., bge-m3)
- Retriever: MultiQueryRetriever for better context coverage
- LLM: ChatOllama using qwen3 model or your prefered llm

## ğŸ§  Prompt Flow

    User Query
        â†“
    MultiQuery Prompt
        â†“
    Relevant Docs Retrieved
        â†“
    Injected into Prompt
        â†“
    LLM Response

## ğŸ“Œ Configuration

All critical settings are in constants.py:

    DATA_PATH = "./data"
    MODEL_NAME = "qwen3" or your-llm
    EMBEDDING_MODEL = "bge-m3" or your-embedder
    SUPPORTED_EXTENSIONS = {".pdf", ".txt", ".md", ".docx"}

## ğŸ““ Notes

    Ensure Ollama is running before starting the app

    All vector data is stored in index/ and reused across sessions
